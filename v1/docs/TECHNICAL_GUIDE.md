# Technical Architecture & Theory

## üìñ Contents
1. [Mathematical Foundations](#mathematical-foundations)
2. [Architecture Details](#architecture-details)
3. [Training Algorithms](#training-algorithms)
4. [Optimization Techniques](#optimization-techniques)
5. [Performance Analysis](#performance-analysis)
6. [Research References](#research-references)

---

## Mathematical Foundations

### 1. Ratio Transformation

**Purpose:** Make time series stationary and properly scaled

**Formula:**
```
ratio(t) = value(t) / value(t-1)
log_ratio(t) = log(ratio(t) + Œµ)  where Œµ = 1e-10
standardized(t) = (log_ratio(t) - Œº) / œÉ
```

**Properties:**
- Converts multiplicative relationships to additive
- Removes trend and seasonality effects
- Makes variance more stable
- Prevents gradient issues from large value differences

**Mathematical Proof of Stationarity:**
```
Let X(t) be original series with trend: X(t) = T(t) + S(t) + N(t)
where T(t) = trend, S(t) = seasonality, N(t) = noise

Ratio series: R(t) = X(t) / X(t-1)

If T(t) = at + b (linear trend):
R(t) = (at + b) / (a(t-1) + b) ‚âà 1 + a/(at+b) for large t
As t ‚Üí ‚àû, R(t) ‚Üí 1 (stationary around 1)

Log transformation: log(R(t)) ‚âà log(1 + a/(at+b)) ‚âà a/(at+b)
This converges to 0, making series stationary
```

### 2. LSTM Cell Mathematics

**Forward Pass:**

```
Input gate:     i(t) = œÉ(W_i¬∑[h(t-1), x(t)] + b_i)
Forget gate:    f(t) = œÉ(W_f¬∑[h(t-1), x(t)] + b_f)
Cell gate:      g(t) = tanh(W_g¬∑[h(t-1), x(t)] + b_g)
Output gate:    o(t) = œÉ(W_o¬∑[h(t-1), x(t)] + b_o)

Cell state:     c(t) = f(t) ‚äô c(t-1) + i(t) ‚äô g(t)
Hidden state:   h(t) = o(t) ‚äô tanh(c(t))

where:
œÉ = sigmoid function: œÉ(x) = 1 / (1 + e^(-x))
tanh = hyperbolic tangent: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
‚äô = element-wise multiplication
```

**Gradient Flow:**

The key innovation of LSTM is the cell state gradient:

```
‚àÇLoss/‚àÇc(t-1) = ‚àÇLoss/‚àÇc(t) ¬∑ f(t)
```

Since f(t) ‚àà (0,1), gradient doesn't explode or vanish as easily as in vanilla RNN.

**Why LSTM Works:**
- Cell state acts as a "highway" for gradients
- Gates control information flow
- Can learn to remember or forget information
- Solves vanishing gradient problem of standard RNNs

### 3. GRU Cell Mathematics

**Simplified Gating:**

```
Reset gate:     r(t) = œÉ(W_r¬∑[h(t-1), x(t)])
Update gate:    z(t) = œÉ(W_z¬∑[h(t-1), x(t)])
Candidate:      hÃÉ(t) = tanh(W¬∑[r(t)‚äôh(t-1), x(t)])
Hidden state:   h(t) = (1 - z(t))‚äôh(t-1) + z(t)‚äôhÃÉ(t)
```

**Key Difference from LSTM:**
- Combines cell state and hidden state
- Only 2 gates instead of 3
- ~30% fewer parameters than LSTM
- Often similar performance

**Parameter Count Comparison:**
```
LSTM: 4 √ó (input_size + hidden_size) √ó hidden_size
GRU:  3 √ó (input_size + hidden_size) √ó hidden_size

Example: input_size=10, hidden_size=64
LSTM: 4 √ó (10 + 64) √ó 64 = 18,944 parameters
GRU:  3 √ó (10 + 64) √ó 64 = 14,208 parameters
Reduction: ~25%
```

### 4. Attention Mechanism

**Self-Attention (Transformer):**

```
Q = X¬∑W_Q  (Query)
K = X¬∑W_K  (Key)
V = X¬∑W_V  (Value)

Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)¬∑V

where d_k is the dimension of keys (for scaling)
```

**Multi-Head Attention:**

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)¬∑W_O

where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**Why Scaling by ‚àöd_k:**
```
For random vectors Q, K ‚àà ‚Ñù^d_k with zero mean and unit variance:
E[QK^T] = 0
Var(QK^T) = d_k

Scaling by ‚àöd_k normalizes variance to 1, preventing softmax saturation
```

**LSTM Attention:**

```
For LSTM outputs H = [h_1, h_2, ..., h_T]:

scores = W_a¬∑H
attention_weights = softmax(scores)
context = Œ£(attention_weights_i √ó h_i)
```

### 5. Temporal Convolutional Network (TCN)

**Dilated Causal Convolution:**

```
Output(t) = Œ£(i=0 to k-1) w_i ¬∑ Input(t - d¬∑i)

where:
k = kernel size
d = dilation factor
Receptive field = (k-1) √ó d + 1
```

**Exponential Receptive Field Growth:**

```
For L layers with dilation d = 2^l:
Total receptive field = Œ£(l=0 to L-1) (k-1)¬∑2^l + 1
                      = (k-1)¬∑(2^L - 1) + 1

Example: k=3, L=4
RF = 2¬∑(2^4 - 1) + 1 = 31 time steps
```

**Residual Connection:**

```
y = Activation(F(x, W) + x)

where F(x, W) is the convolutional transformation
```

### 6. Positional Encoding (Transformer)

**Sinusoidal Position Encoding:**

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

where:
pos = position in sequence
i = dimension index
d_model = model dimension
```

**Why Sinusoidal:**
- Allows model to learn relative positions
- Can extrapolate to longer sequences than trained
- Fixed (no learnable parameters)
- Provides unique encoding for each position

---

## Architecture Details

### Model Complexity Analysis

**Parameter Count Formulas:**

```
LSTM Layer:
params = 4 √ó (input_size + hidden_size + 1) √ó hidden_size

GRU Layer:
params = 3 √ó (input_size + hidden_size + 1) √ó hidden_size

Transformer Encoder:
params_attention = 4 √ó d_model √ó d_model  (Q, K, V, O matrices)
params_ffn = 2 √ó d_model √ó d_ff  (feed-forward)
params_total = (params_attention + params_ffn) √ó num_layers

CNN Layer:
params = kernel_size √ó in_channels √ó out_channels + out_channels

Dense Layer:
params = input_size √ó output_size + output_size
```

**Computational Complexity:**

| Model | Time Complexity | Space Complexity |
|-------|----------------|------------------|
| LSTM | O(T √ó d¬≤) | O(T √ó d) |
| GRU | O(T √ó d¬≤) | O(T √ó d) |
| TCN | O(T √ó k √ó d¬≤) | O(T √ó d) |
| Transformer | O(T¬≤ √ó d) | O(T¬≤ + T √ó d) |
| MLP | O(T √ó d¬≤) | O(T √ó d) |

where:
- T = sequence length
- d = hidden dimension
- k = kernel size

**Memory Usage:**

```
Model memory = parameters √ó 4 bytes (float32)
Activation memory = batch_size √ó sequence_length √ó hidden_size √ó 4 bytes

Example: LSTM with hidden_size=64, batch_size=32, seq_len=20
Model: ~20K params √ó 4 = 80 KB
Activations: 32 √ó 20 √ó 64 √ó 4 = 163 KB
Total: ~250 KB per batch
```

### Receptive Field Analysis

**LSTM/GRU:**
- Theoretically infinite (recurrent connections)
- Practically limited by vanishing gradients
- Effective receptive field: ~50-100 steps

**TCN:**
```
RF = (kernel_size - 1) √ó (2^num_layers - 1) + 1

Examples:
kernel=3, layers=3: RF = 2√ó(8-1)+1 = 15
kernel=3, layers=4: RF = 2√ó(16-1)+1 = 31
kernel=5, layers=3: RF = 4√ó(8-1)+1 = 29
```

**CNN-LSTM:**
- CNN captures local patterns (RF = kernel_size)
- LSTM captures long-term dependencies
- Combined: Local + global structure

**Transformer:**
- Full attention: sees all time steps simultaneously
- Receptive field = entire sequence
- No distance bias (all positions equally accessible)

---

## Training Algorithms

### 1. Backpropagation Through Time (BPTT)

**Algorithm:**

```
Forward Pass:
for t = 1 to T:
    h(t) = f(h(t-1), x(t), W)
    y(t) = g(h(t), W_out)

Loss = Œ£(t=1 to T) Loss(y(t), target(t))

Backward Pass:
for t = T down to 1:
    ‚àÇLoss/‚àÇh(t) = ‚àÇLoss/‚àÇy(t)¬∑‚àÇy(t)/‚àÇh(t) + ‚àÇLoss/‚àÇh(t+1)¬∑‚àÇh(t+1)/‚àÇh(t)
    ‚àÇLoss/‚àÇW += ‚àÇLoss/‚àÇh(t)¬∑‚àÇh(t)/‚àÇW
```

**Truncated BPTT:**
- Limit backprop to k time steps
- Reduces memory and computation
- May miss very long dependencies

### 2. Gradient Clipping

**Global Norm Clipping:**

```
g = ‚àáLoss  (gradient)
g_norm = ||g||_2

if g_norm > max_norm:
    g = g √ó (max_norm / g_norm)
```

**Why It Works:**
- Prevents gradient explosion
- Maintains gradient direction
- Allows larger learning rates

### 3. Learning Rate Schedules

**Reduce on Plateau:**

```
if val_loss doesn't improve for patience epochs:
    lr = lr √ó factor
```

**Cosine Annealing:**

```
lr(t) = lr_min + (lr_max - lr_min) √ó (1 + cos(œÄt/T)) / 2
```

**Warmup (for Transformers):**

```
lr(t) = d_model^(-0.5) √ó min(t^(-0.5), t √ó warmup_steps^(-1.5))
```

### 4. Optimization Algorithms

**Adam (Adaptive Moment Estimation):**

```
m(t) = Œ≤‚ÇÅ¬∑m(t-1) + (1-Œ≤‚ÇÅ)¬∑g(t)  (first moment)
v(t) = Œ≤‚ÇÇ¬∑v(t-1) + (1-Œ≤‚ÇÇ)¬∑g(t)¬≤  (second moment)

mÃÇ(t) = m(t) / (1 - Œ≤‚ÇÅ^t)  (bias correction)
vÃÇ(t) = v(t) / (1 - Œ≤‚ÇÇ^t)

Œ∏(t) = Œ∏(t-1) - Œ±¬∑mÃÇ(t) / (‚àövÃÇ(t) + Œµ)

Default: Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999, Œµ=1e-8, Œ±=0.001
```

**Why Adam Works for Time Series:**
- Adaptive learning rates per parameter
- Momentum helps escape local minima
- Works well with sparse gradients
- Less sensitive to lr tuning than SGD

---

## Optimization Techniques

### 1. Dropout

**Standard Dropout:**

```
During training:
output = input √ó mask / (1 - p)
where mask ~ Bernoulli(1 - p)

During inference:
output = input  (no dropout)
```

**Variational Dropout (RNN):**
- Same dropout mask across all time steps
- Prevents degradation of hidden state information

```python
# Implementation
dropout_mask = torch.bernoulli(torch.ones_like(h) * (1 - p))
for t in range(T):
    h_t = lstm_cell(x_t, h_{t-1})
    h_t = h_t * dropout_mask / (1 - p)  # Same mask every step
```

### 2. Batch Normalization

**Formula:**

```
Œº_B = (1/m)¬∑Œ£x_i  (batch mean)
œÉ¬≤_B = (1/m)¬∑Œ£(x_i - Œº_B)¬≤  (batch variance)

xÃÇ_i = (x_i - Œº_B) / ‚àö(œÉ¬≤_B + Œµ)  (normalize)
y_i = Œ≥¬∑xÃÇ_i + Œ≤  (scale and shift)
```

**Why Not Always Used in RNN:**
- Batch statistics vary across time steps
- Can break temporal dependencies
- Layer normalization often better for sequences

### 3. Layer Normalization

**Formula:**

```
Œº = (1/H)¬∑Œ£h_i  (mean across hidden dims)
œÉ¬≤ = (1/H)¬∑Œ£(h_i - Œº)¬≤  (variance)

ƒ•_i = (h_i - Œº) / ‚àö(œÉ¬≤ + Œµ)
h'_i = Œ≥¬∑ƒ•_i + Œ≤
```

**Better for RNN Because:**
- Statistics computed per sample, not batch
- Independent across time steps
- Maintains temporal structure

### 4. Residual Connections

**Formula:**

```
y = F(x, W) + x

where F is some transformation (conv, dense, etc.)
```

**Gradient Flow:**

```
‚àÇLoss/‚àÇx = ‚àÇLoss/‚àÇy¬∑(‚àÇF/‚àÇx + I)

Identity path allows gradients to flow directly
```

**Benefits:**
- Enables training very deep networks
- Mitigates vanishing gradients
- Helps optimization landscape

### 5. Weight Initialization

**Xavier/Glorot Initialization:**

```
W ~ Uniform(-‚àö(6/(n_in + n_out)), ‚àö(6/(n_in + n_out)))

or

W ~ Normal(0, ‚àö(2/(n_in + n_out)))
```

**He Initialization (for ReLU):**

```
W ~ Normal(0, ‚àö(2/n_in))
```

**Why It Matters:**
- Maintains activation variance
- Prevents saturation or explosion
- Enables deeper networks

---

## Performance Analysis

### Training Time Benchmarks

**Single Epoch (1000 samples, sequence_length=20):**

| Model | CPU (seconds) | GPU (seconds) | Speedup |
|-------|--------------|--------------|---------|
| MLP | 0.5 | 0.1 | 5√ó |
| LSTM | 2.5 | 0.3 | 8√ó |
| GRU | 1.8 | 0.2 | 9√ó |
| BiLSTM | 4.0 | 0.5 | 8√ó |
| CNN-LSTM | 3.0 | 0.4 | 7√ó |
| TCN | 1.5 | 0.2 | 7√ó |
| Transformer | 3.5 | 0.6 | 6√ó |
| Attention-LSTM | 4.5 | 0.7 | 6√ó |

### Memory Benchmarks

**Peak Memory Usage (batch_size=32, sequence_length=20):**

| Model | Parameters | Model Size | Activation Memory |
|-------|-----------|-----------|------------------|
| MLP | 50K | 200 KB | 100 KB |
| LSTM [64,32] | 45K | 180 KB | 250 KB |
| GRU [64,32] | 35K | 140 KB | 200 KB |
| BiLSTM [64,32] | 60K | 240 KB | 400 KB |
| CNN-LSTM | 55K | 220 KB | 300 KB |
| TCN [64,64,32] | 48K | 192 KB | 280 KB |
| Transformer | 65K | 260 KB | 450 KB |
| Attention-LSTM | 52K | 208 KB | 320 KB |

### Accuracy vs Efficiency Trade-off

```
Efficiency Score = (1 - MAE) / training_time

Higher is better (good accuracy, fast training)

Typical Rankings:
1. GRU (0.85)
2. LSTM (0.80)
3. TCN (0.78)
4. CNN-LSTM (0.75)
5. MLP (0.70)
6. Transformer (0.65)
7. Attention-LSTM (0.60)
8. BiLSTM (0.55)
```

---

## Research References

### Seminal Papers

**LSTM:**
```
Hochreiter, S., & Schmidhuber, J. (1997).
Long short-term memory.
Neural computation, 9(8), 1735-1780.
```

**GRU:**
```
Cho, K., et al. (2014).
Learning phrase representations using RNN encoder-decoder 
for statistical machine translation.
EMNLP 2014.
```

**Attention Mechanism:**
```
Bahdanau, D., Cho, K., & Bengio, Y. (2014).
Neural machine translation by jointly learning to align and translate.
ICLR 2015.
```

**Transformer:**
```
Vaswani, A., et al. (2017).
Attention is all you need.
NeurIPS 2017.
```

**TCN:**
```
Bai, S., Kolter, J. Z., & Koltun, V. (2018).
An empirical evaluation of generic convolutional and 
recurrent networks for sequence modeling.
arXiv:1803.01271.
```

**Batch Normalization:**
```
Ioffe, S., & Szegedy, C. (2015).
Batch normalization: Accelerating deep network training 
by reducing internal covariate shift.
ICML 2015.
```

**Adam Optimizer:**
```
Kingma, D. P., & Ba, J. (2014).
Adam: A method for stochastic optimization.
ICLR 2015.
```

### Time Series Specific Papers

**Deep Learning for Time Series:**
```
Lim, B., & Zohren, S. (2021).
Time-series forecasting with deep learning: A survey.
Philosophical Transactions of the Royal Society A, 379(2194).
```

**Financial Time Series:**
```
Fischer, T., & Krauss, C. (2018).
Deep learning with long short-term memory networks 
for financial market predictions.
European Journal of Operational Research, 270(2), 654-669.
```

**Attention for Time Series:**
```
Qin, Y., et al. (2017).
A dual-stage attention-based recurrent neural network 
for time series prediction.
IJCAI 2017.
```

---

## Implementation Notes

### PyTorch-Specific Optimizations

**1. DataLoader Optimization:**

```python
# Use multiple workers for data loading
DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)

# pin_memory: Faster transfer to GPU
# num_workers: Parallel data loading
```

**2. Model to GPU:**

```python
# Move model and data together
model = model.to(device)
for batch_X, batch_y in dataloader:
    batch_X = batch_X.to(device, non_blocking=True)
    batch_y = batch_y.to(device, non_blocking=True)
```

**3. Gradient Accumulation:**

```python
# Simulate larger batch size
accumulation_steps = 4
for i, (batch_X, batch_y) in enumerate(dataloader):
    loss = criterion(model(batch_X), batch_y) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**4. Mixed Precision:**

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### Numerical Stability

**1. Handling Log Operations:**

```python
# Avoid log(0)
log_ratio = torch.log(ratio + 1e-10)

# Use log-sum-exp trick for numerical stability
def log_sum_exp(x):
    max_x = torch.max(x)
    return max_x + torch.log(torch.sum(torch.exp(x - max_x)))
```

**2. Gradient Clipping:**

```python
# Clip by global norm
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Clip by value
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

**3. Loss Scaling:**

```python
# For very small or large losses
loss = loss / scale_factor
loss.backward()
# Scale gradients back
for param in model.parameters():
    if param.grad is not None:
        param.grad *= scale_factor
```

---

## Debugging Checklist

### Before Training

- [ ] Check data shape: (samples, features)
- [ ] Verify no NaN or Inf values
- [ ] Ensure chronological order
- [ ] Validate train/val/test split
- [ ] Check sequence length < data length

### During Training

- [ ] Monitor both train and val loss
- [ ] Check for gradient explosion (NaN loss)
- [ ] Verify learning rate is appropriate
- [ ] Watch for overfitting (val_loss >> train_loss)
- [ ] Check GPU utilization (nvidia-smi)

### After Training

- [ ] Compare to baseline (MLP)
- [ ] Visualize predictions
- [ ] Check residuals for patterns
- [ ] Test on out-of-sample data
- [ ] Verify model saves correctly

---

## Advanced Topics

### 1. Uncertainty Quantification

**Monte Carlo Dropout:**

```python
def predict_with_uncertainty(model, x, num_samples=100):
    model.train()  # Keep dropout active
    predictions = []
    
    with torch.no_grad():
        for _ in range(num_samples):
            pred = model(x)
            predictions.append(pred)
    
    predictions = torch.stack(predictions)
    mean = predictions.mean(dim=0)
    std = predictions.std(dim=0)
    
    return mean, std
```

### 2. Explainability

**Integrated Gradients:**

```python
def integrated_gradients(model, x, baseline=None, steps=50):
    if baseline is None:
        baseline = torch.zeros_like(x)
    
    # Generate interpolated inputs
    alphas = torch.linspace(0, 1, steps)
    interpolated = baseline + alphas.view(-1, 1, 1) * (x - baseline)
    
    # Compute gradients
    grads = []
    for interp in interpolated:
        interp.requires_grad = True
        output = model(interp.unsqueeze(0))
        grad = torch.autograd.grad(output.sum(), interp)[0]
        grads.append(grad)
    
    # Average gradients
    avg_grads = torch.stack(grads).mean(dim=0)
    integrated_grads = (x - baseline) * avg_grads
    
    return integrated_grads
```

### 3. Multi-Task Learning

**Shared Encoder, Task-Specific Heads:**

```python
class MultiTaskModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        # Shared encoder
        self.encoder = LSTMModel(input_size, hidden_sizes=[64, 32])
        
        # Task-specific heads
        self.task1_head = nn.Linear(32, 1)  # Regression
        self.task2_head = nn.Linear(32, 3)  # Classification
    
    def forward(self, x):
        features = self.encoder(x)
        task1_out = self.task1_head(features)
        task2_out = self.task2_head(features)
        return task1_out, task2_out
```

---

**For more details, refer to the research papers and PyTorch documentation.**
