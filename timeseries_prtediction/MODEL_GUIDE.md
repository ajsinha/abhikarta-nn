# Complete Guide to Time Series Models

## ğŸ“š Table of Contents
1. [Model Overview](#model-overview)
2. [Detailed Model Descriptions](#detailed-model-descriptions)
3. [When to Use Each Model](#when-to-use-each-model)
4. [Performance Comparison](#performance-comparison)
5. [Usage Examples](#usage-examples)
6. [Hyperparameter Tuning](#hyperparameter-tuning)
7. [Best Practices](#best-practices)

---

## Model Overview

This package includes **8 different neural network architectures** for time series prediction:

| Model | Type | Complexity | Speed | Best For |
|-------|------|------------|-------|----------|
| **LSTM** | Recurrent | Medium | Medium | General-purpose, long dependencies |
| **GRU** | Recurrent | Medium | Fast | Faster alternative to LSTM |
| **BiLSTM** | Recurrent | High | Slow | Pattern recognition, no real-time |
| **CNN-LSTM** | Hybrid | High | Medium | Multi-scale patterns |
| **TCN** | Convolutional | Medium | Fast | Long sequences, parallel processing |
| **Transformer** | Attention | High | Medium | Complex patterns, long-range |
| **Attention-LSTM** | Recurrent + Attention | High | Slow | Interpretable predictions |
| **MLP** | Feedforward | Low | Very Fast | Baseline comparison |

---

## Detailed Model Descriptions

### 1. LSTM (Long Short-Term Memory)

**Architecture:**
```
Input â†’ LSTM Layer 1 â†’ Dropout â†’ LSTM Layer 2 â†’ Dropout â†’ Dense â†’ Output
```

**How it works:**
- Uses "gates" (input, forget, output) to control information flow
- Maintains a "cell state" to remember long-term dependencies
- Can learn which information to keep and which to forget

**Key Features:**
- âœ… Handles vanishing gradient problem
- âœ… Captures long-term dependencies
- âœ… Standard choice for time series
- âŒ Slower than GRU
- âŒ More parameters to train

**Mathematical Formulation:**
```
forget gate: f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
input gate:  i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
cell state:  c_t = f_t * c_{t-1} + i_t * tanh(W_c Â· [h_{t-1}, x_t] + b_c)
output gate: o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
hidden:      h_t = o_t * tanh(c_t)
```

**Hyperparameters:**
```python
LSTMModel(
    input_size=10,           # Number of features
    hidden_sizes=[64, 32],   # Size of each LSTM layer
    dropout=0.2              # Dropout rate (0.0-0.5)
)
```

**When to Use:**
- âœ“ Default choice for most time series problems
- âœ“ When you need to capture long-term patterns
- âœ“ Sequential dependencies are important
- âœ“ You have sufficient training data (500+ samples)

**When NOT to Use:**
- âœ— When speed is critical (use GRU or TCN)
- âœ— Very short sequences (use MLP)
- âœ— Real-time constraints (consider TCN)

---

### 2. GRU (Gated Recurrent Unit)

**Architecture:**
```
Input â†’ GRU Layer 1 â†’ Dropout â†’ GRU Layer 2 â†’ Dropout â†’ Dense â†’ Output
```

**How it works:**
- Simplified version of LSTM with only 2 gates (reset, update)
- Combines cell state and hidden state into one
- Generally faster than LSTM with similar performance

**Key Features:**
- âœ… Faster training than LSTM (30-40% speedup)
- âœ… Fewer parameters
- âœ… Often performs as well as LSTM
- âŒ May struggle with very long dependencies

**Mathematical Formulation:**
```
reset gate:  r_t = Ïƒ(W_r Â· [h_{t-1}, x_t])
update gate: z_t = Ïƒ(W_z Â· [h_{t-1}, x_t])
candidate:   hÌƒ_t = tanh(W Â· [r_t * h_{t-1}, x_t])
hidden:      h_t = (1 - z_t) * h_{t-1} + z_t * hÌƒ_t
```

**Hyperparameters:**
```python
GRUModel(
    input_size=10,
    hidden_sizes=[64, 32],
    dropout=0.2
)
```

**When to Use:**
- âœ“ When training speed matters
- âœ“ Large datasets where LSTM is too slow
- âœ“ Memory constraints (fewer parameters)
- âœ“ As a faster alternative to LSTM

**When NOT to Use:**
- âœ— When you need absolute best performance
- âœ— Very long-term dependencies (>100 time steps)

---

### 3. Bidirectional LSTM (BiLSTM)

**Architecture:**
```
Input â†’ BiLSTM (forward + backward) â†’ LSTM â†’ Dropout â†’ Dense â†’ Output
```

**How it works:**
- Processes sequence in both forward and backward directions
- Combines information from past and future
- Better feature extraction at each time step

**Key Features:**
- âœ… Captures patterns from both directions
- âœ… Better feature representation
- âœ… Often improves accuracy
- âŒ Cannot be used for real-time prediction
- âŒ Slower (processes sequence twice)

**When to Use:**
- âœ“ Post-hoc analysis (not real-time)
- âœ“ Pattern recognition in historical data
- âœ“ Classification tasks
- âœ“ When you have the full sequence available

**When NOT to Use:**
- âœ— Real-time predictions (needs future data)
- âœ— Online learning scenarios
- âœ— Stream processing

---

### 4. CNN-LSTM Hybrid

**Architecture:**
```
Input â†’ Conv1D â†’ MaxPool â†’ Conv1D â†’ LSTM â†’ Dense â†’ Output
```

**How it works:**
- CNN layers extract local features and patterns
- Pooling reduces dimensionality
- LSTM captures temporal dependencies in extracted features
- Combines spatial and temporal processing

**Key Features:**
- âœ… Efficient local feature extraction
- âœ… Good for high-frequency data
- âœ… Multi-scale pattern recognition
- âŒ More complex to tune
- âŒ Requires understanding of both CNN and LSTM

**Hyperparameters:**
```python
CNNLSTMModel(
    input_size=10,
    cnn_filters=64,      # Number of CNN filters
    lstm_hidden=64,      # LSTM hidden size
    dropout=0.2
)
```

**When to Use:**
- âœ“ High-frequency time series (stock prices, sensor data)
- âœ“ Both local and global patterns are important
- âœ“ Data has multi-scale structure
- âœ“ Images or spectrograms as input

**When NOT to Use:**
- âœ— Low-frequency data
- âœ— Very short sequences
- âœ— When interpretability is critical

---

### 5. Temporal Convolutional Network (TCN)

**Architecture:**
```
Input â†’ Temporal Block 1 â†’ Temporal Block 2 â†’ ... â†’ Temporal Block N â†’ Dense â†’ Output
```

**Each Temporal Block:**
```
Input â†’ Causal Conv1D â†’ ReLU â†’ Dropout â†’ Causal Conv1D â†’ ReLU â†’ Dropout â†’ (+Residual)
```

**How it works:**
- Uses dilated causal convolutions
- Each layer has exponentially increasing dilation
- Achieves large receptive field efficiently
- Parallel processing unlike RNNs

**Key Features:**
- âœ… Very fast training (fully parallelizable)
- âœ… Long receptive field with fewer layers
- âœ… Stable gradients (no vanishing gradient)
- âœ… Deterministic predictions
- âŒ May need many layers for very long sequences
- âŒ Less intuitive than LSTM

**Hyperparameters:**
```python
TCNModel(
    input_size=10,
    num_channels=[64, 64, 32],  # Channels in each temporal block
    kernel_size=3,               # Convolution kernel size
    dropout=0.2
)
```

**Receptive Field:**
```
receptive_field = 2^n * (kernel_size - 1) + 1
# With 3 layers, kernel_size=3: 2^3 * 2 + 1 = 17
```

**When to Use:**
- âœ“ Long sequences (100+ time steps)
- âœ“ Need fast training/inference
- âœ“ Real-time applications
- âœ“ When you want stable training

**When NOT to Use:**
- âœ— Very short sequences
- âœ— When RNN structure is proven better for your domain

---

### 6. Transformer

**Architecture:**
```
Input â†’ Linear Projection â†’ Positional Encoding â†’ 
Transformer Encoder Blocks â†’ Dense â†’ Output
```

**Each Encoder Block:**
```
Input â†’ Multi-Head Self-Attention â†’ Add & Norm â†’ 
Feed Forward â†’ Add & Norm â†’ Output
```

**How it works:**
- Uses self-attention to weigh importance of different time steps
- Positional encoding adds sequence order information
- Parallel processing of entire sequence
- Can capture complex, long-range dependencies

**Key Features:**
- âœ… Captures complex patterns
- âœ… Handles very long sequences
- âœ… Highly parallelizable
- âœ… State-of-the-art in many domains
- âŒ Requires lots of data (1000+ samples)
- âŒ Computationally expensive
- âŒ Many hyperparameters to tune

**Hyperparameters:**
```python
TransformerModel(
    input_size=10,
    d_model=64,          # Model dimension
    nhead=4,             # Number of attention heads
    num_layers=2,        # Number of encoder layers
    dropout=0.2
)
```

**Attention Mechanism:**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

**When to Use:**
- âœ“ Large datasets (1000+ samples)
- âœ“ Complex temporal patterns
- âœ“ Long-range dependencies crucial
- âœ“ State-of-the-art performance needed
- âœ“ Parallel processing available

**When NOT to Use:**
- âœ— Small datasets (<500 samples)
- âœ— Limited computational resources
- âœ— Simple patterns
- âœ— Need fast prototyping

---

### 7. Attention-LSTM

**Architecture:**
```
Input â†’ LSTM Layers â†’ Attention Mechanism â†’ Dense â†’ Output
```

**How it works:**
- LSTM processes sequence and outputs hidden states for all time steps
- Attention mechanism learns which time steps are most important
- Weighted combination of all time steps used for prediction
- More interpretable than standard LSTM

**Key Features:**
- âœ… Interpretable (can visualize attention weights)
- âœ… Often better performance than vanilla LSTM
- âœ… Focuses on important time steps
- âŒ Slightly slower than LSTM
- âŒ More parameters

**Attention Weights:**
```
Î±_t = exp(score(h_t)) / Î£ exp(score(h_i))
context = Î£ Î±_t * h_t
```

**Hyperparameters:**
```python
AttentionLSTMModel(
    input_size=10,
    hidden_size=64,
    dropout=0.2
)
```

**When to Use:**
- âœ“ Need to understand which time steps matter
- âœ“ Interpretability is important
- âœ“ Variable-length sequences
- âœ“ When some time steps are more informative

**When NOT to Use:**
- âœ— Speed is critical
- âœ— Simple patterns
- âœ— Very short sequences

---

### 8. MLP (Multi-Layer Perceptron)

**Architecture:**
```
Input (flattened) â†’ Dense Layer 1 â†’ ReLU â†’ Dropout â†’ 
Dense Layer 2 â†’ ReLU â†’ Dropout â†’ Dense Layer 3 â†’ Output
```

**How it works:**
- Flattens the sequence into a single vector
- Processes with standard feedforward layers
- No explicit temporal modeling
- Very simple and fast

**Key Features:**
- âœ… Very fast training and inference
- âœ… Simple to understand and debug
- âœ… Good baseline for comparison
- âŒ Doesn't capture temporal structure
- âŒ Fixed sequence length required
- âŒ Limited capacity for complex patterns

**Hyperparameters:**
```python
MLPModel(
    input_size=10,
    sequence_length=20,
    hidden_sizes=[128, 64, 32],
    dropout=0.2
)
```

**When to Use:**
- âœ“ As a baseline for comparison
- âœ“ Very simple patterns
- âœ“ When speed is critical
- âœ“ Debugging other models

**When NOT to Use:**
- âœ— Complex temporal dependencies
- âœ— Long sequences
- âœ— When temporal structure matters
- âœ— Production systems (usually outperformed)

---

## When to Use Each Model

### Decision Tree

```
Do you need real-time predictions?
â”‚
â”œâ”€ YES â†’ Do you have long sequences?
â”‚         â”œâ”€ YES â†’ Use TCN
â”‚         â””â”€ NO  â†’ Use GRU or LSTM
â”‚
â””â”€ NO  â†’ Do you have lots of data (1000+ samples)?
          â”œâ”€ YES â†’ Is performance critical?
          â”‚        â”œâ”€ YES â†’ Try Transformer or Attention-LSTM
          â”‚        â””â”€ NO  â†’ Start with LSTM or BiLSTM
          â”‚
          â””â”€ NO  â†’ Do you need fast training?
                   â”œâ”€ YES â†’ Use GRU
                   â””â”€ NO  â†’ Use LSTM
```

### By Use Case

**Financial Time Series (Stock Prices, Forex):**
1st choice: GRU or LSTM
2nd choice: TCN
3rd choice: Attention-LSTM

**High-Frequency Sensor Data:**
1st choice: CNN-LSTM
2nd choice: TCN
3rd choice: GRU

**Long-Term Forecasting (>100 steps ahead):**
1st choice: Transformer
2nd choice: TCN
3rd choice: LSTM

**Resource-Constrained (Edge Devices):**
1st choice: GRU
2nd choice: MLP
3rd choice: TCN

**Research/Experimentation:**
1st choice: Transformer
2nd choice: Attention-LSTM
3rd choice: BiLSTM

**Quick Prototyping:**
1st choice: LSTM
2nd choice: GRU
3rd choice: MLP

---

## Performance Comparison

### Typical Performance on Standard Datasets

| Model | Training Time | Inference Speed | Accuracy | Memory Usage |
|-------|--------------|-----------------|----------|--------------|
| LSTM | Medium | Medium | â˜…â˜…â˜…â˜…â˜† | Medium |
| GRU | Fast | Fast | â˜…â˜…â˜…â˜…â˜† | Low |
| BiLSTM | Slow | Slow | â˜…â˜…â˜…â˜…â˜… | High |
| CNN-LSTM | Medium | Medium | â˜…â˜…â˜…â˜…â˜† | Medium |
| TCN | Fast | Very Fast | â˜…â˜…â˜…â˜…â˜† | Medium |
| Transformer | Slow | Medium | â˜…â˜…â˜…â˜…â˜… | High |
| Attention-LSTM | Slow | Slow | â˜…â˜…â˜…â˜…â˜… | High |
| MLP | Very Fast | Very Fast | â˜…â˜…â˜…â˜†â˜† | Low |

### Scalability

**Small Datasets (<500 samples):**
1. LSTM / GRU
2. MLP
3. CNN-LSTM

**Medium Datasets (500-2000 samples):**
1. LSTM / GRU
2. CNN-LSTM
3. TCN
4. Attention-LSTM

**Large Datasets (>2000 samples):**
1. Transformer
2. TCN
3. CNN-LSTM
4. Attention-LSTM

### Sequence Length

**Short Sequences (10-30 steps):**
- LSTM, GRU, MLP all work well
- Transformer may be overkill

**Medium Sequences (30-100 steps):**
- LSTM, GRU, TCN recommended
- CNN-LSTM for high-frequency data

**Long Sequences (>100 steps):**
- TCN (most efficient)
- Transformer (best accuracy with enough data)
- LSTM (may struggle with very long sequences)

---

## Usage Examples

### Example 1: Quick Start with LSTM

```python
from timeseries_prtediction.timeseries_all_models import *

# Create model
model = create_model('lstm', input_size=10, hidden_sizes=[64, 32])
model = model.to(device)

# Train
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

history, best_state = train_model(
    model, train_loader, val_loader,
    criterion, optimizer, num_epochs=50, device=device
)

# Evaluate
predictions, actuals, metrics = evaluate_model(model, test_loader, device)
print(f"Test RMSE: {metrics['rmse']:.4f}")
```

### Example 2: Compare Multiple Models

```python
models_to_test = ['lstm', 'gru', 'tcn', 'transformer']
results = {}

for model_type in models_to_test:
    model = create_model(model_type, input_size=10)
    model = model.to(device)
    
    # Train and evaluate
    # ... (training code)
    
    results[model_type] = metrics

# Find best model
best = min(results.items(), key=lambda x: x[1]['rmse'])
print(f"Best model: {best[0]} with RMSE: {best[1]['rmse']:.4f}")
```

### Example 3: Using CNN-LSTM for High-Frequency Data

```python
# For data with local patterns
model = CNNLSTMModel(
    input_size=10,
    cnn_filters=64,      # Adjust based on data complexity
    lstm_hidden=64,
    dropout=0.3          # Higher dropout for high-frequency noise
)

# Use larger batch size for stability
train_loader = DataLoader(dataset, batch_size=64, shuffle=True)
```

### Example 4: Transformer for Long Sequences

```python
# Prepare longer sequences
sequence_length = 100

model = TransformerModel(
    input_size=10,
    d_model=128,         # Larger model for complex patterns
    nhead=8,            # More attention heads
    num_layers=3,       # Deeper network
    dropout=0.1
)

# Use learning rate scheduler
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)
```

### Example 5: Attention-LSTM with Visualization

```python
model = AttentionLSTMModel(input_size=10, hidden_size=64)

# After training, visualize attention weights
model.eval()
with torch.no_grad():
    sample_X = test_dataset[0][0].unsqueeze(0).to(device)
    output = model(sample_X)
    
    # Get attention weights from the model
    lstm_out, _ = model.lstm(sample_X)
    context, attention_weights = model.attention(lstm_out)
    
    # Plot attention weights
    plt.figure(figsize=(10, 4))
    plt.plot(attention_weights[0].cpu().numpy())
    plt.title('Attention Weights Over Time')
    plt.xlabel('Time Step')
    plt.ylabel('Attention Weight')
    plt.show()
```

---

## Hyperparameter Tuning

### General Guidelines

**Learning Rate:**
- Start: 0.001
- Range: 0.0001 to 0.01
- Use scheduler for longer training

**Dropout:**
- Low noise: 0.1-0.2
- High noise: 0.3-0.5
- Prevent overfitting: increase dropout

**Batch Size:**
- Small datasets: 16-32
- Large datasets: 64-128
- GPU memory limited: reduce batch size

**Sequence Length:**
- Start: 20-30
- Financial data: 20-50
- Seasonal data: match seasonality period
- Longer = more context but slower

### Model-Specific Tuning

**LSTM/GRU:**
```python
# Start simple
hidden_sizes=[32]

# If underfitting
hidden_sizes=[64, 32]

# If still underfitting
hidden_sizes=[128, 64, 32]
```

**TCN:**
```python
# Adjust receptive field
num_channels=[32, 32, 32]  # Moderate receptive field
num_channels=[64, 64, 64, 64]  # Larger receptive field

# Kernel size affects receptive field exponentially
kernel_size=3  # Standard
kernel_size=5  # Larger patterns
```

**Transformer:**
```python
# Balance between model capacity and overfitting
d_model=64, nhead=4, num_layers=2  # Small dataset
d_model=128, nhead=8, num_layers=3  # Medium dataset
d_model=256, nhead=8, num_layers=4  # Large dataset
```

---

## Best Practices

### 1. Data Preparation
- âœ… Remove outliers carefully
- âœ… Check for missing values
- âœ… Ensure chronological order
- âœ… Use ratio transformation for stationarity
- âœ… Split data temporally, not randomly

### 2. Model Selection
- âœ… Start simple (LSTM/GRU)
- âœ… Try multiple models
- âœ… Use MLP as baseline
- âœ… Consider computational constraints
- âœ… Match model to problem complexity

### 3. Training
- âœ… Use early stopping
- âœ… Monitor both train and validation loss
- âœ… Save best model
- âœ… Use learning rate scheduling
- âœ… Try different random seeds

### 4. Evaluation
- âœ… Use multiple metrics (MSE, MAE, RMSE)
- âœ… Walk-forward validation for time series
- âœ… Visualize predictions
- âœ… Check residuals for patterns
- âœ… Test on out-of-sample data

### 5. Production
- âœ… Version control models
- âœ… Monitor performance drift
- âœ… Plan for retraining
- âœ… Document assumptions
- âœ… A/B test new models

---

## Common Pitfalls

### âŒ Using Random Train/Test Split
**Problem:** Leaks future information
**Solution:** Use temporal split

### âŒ Not Checking for Data Leakage
**Problem:** Model sees future in features
**Solution:** Ensure all features use only past data

### âŒ Over-Complicating Early
**Problem:** Complex model without baseline
**Solution:** Start with LSTM, compare to MLP baseline

### âŒ Ignoring Validation Loss
**Problem:** Overfitting goes unnoticed
**Solution:** Always monitor validation metrics

### âŒ Using Too Long Sequences
**Problem:** Slower training, diminishing returns
**Solution:** Experiment with different lengths

---

## Quick Reference

### Model Selection Cheat Sheet

| Requirement | Recommended Model |
|-------------|------------------|
| Fast training | GRU, MLP |
| Best accuracy (large data) | Transformer, Attention-LSTM |
| Interpretability | Attention-LSTM |
| Real-time prediction | GRU, TCN |
| Long sequences | TCN, Transformer |
| Limited data | LSTM, GRU |
| High-frequency data | CNN-LSTM, TCN |
| Baseline | MLP |

### Quick Commands

```bash
# Run all models comparison
python timeseries_all_models.py

# Train specific model
model = create_model('lstm', input_size=10)

# Available models
['lstm', 'gru', 'bilstm', 'cnn_lstm', 'tcn', 'transformer', 'attention_lstm', 'mlp']
```

---

**Remember:** The best model depends on your specific data and requirements. Always experiment with multiple approaches!
